{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "А теперь серьезно...\n",
    "\n",
    "### Формат файлов\n",
    "\n",
    "Вам выдается 4 файла:\n",
    "\n",
    "* `train.txt` --- обучающая выборка пар запрос-документ и асессорские метки релевантности\n",
    "* `test.txt` --- тестовая выборка пар запрос-документ\n",
    "* `queries_test.txt` --- запросы из `train.txt`\n",
    "* `queries_train.txt` --- запросы из `test.txt`\n",
    "\n",
    "Колонки в файлах могут быть следующего типа:\n",
    "\n",
    "* `QueryId` --- уникальный номер запроса\n",
    "* `DocumentId` --- номер документа, не повторяется для одного запроса\n",
    "* `DocumentLink` --- url документа\n",
    "* `Relevance` --- асессорская метка релевантности\n",
    "\n",
    "Формат файла ответов приведен ниже. Пары запрос-документ должны соответсвовать файлу `test.txt` и должны быть упорядочены по убыванию построенной функции релевантности. То есть так, как в поисковой выдаче."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "QueryId,DocumentId\n",
    "101,5\n",
    "101,0\n",
    "101,9\n",
    "101,13\n",
    "101,17\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество оценивается с помощью метрики ${NDCG}_{10}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('queries_train.txt') as f:\n",
    "    queries = f.readlines()\n",
    "queries = list(map(lambda x: x[:-1].split('\\t')[1], queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def dcg_score(y_true, k=10):\n",
    "#     order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "\n",
    "    gain = 2 ** y_true - 1\n",
    "\n",
    "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
    "    return np.sum(gain / discounts)\n",
    "\n",
    "\n",
    "def ndcg_score(y_true, y_pred, k=10):\n",
    "#     lb = LabelBinarizer()\n",
    "#     T = lb.fit_transform(y_true)\n",
    "    actual = dcg_score(y_pred, k)\n",
    "    best = dcg_score(y_true, k)\n",
    "    return actual * 1. / best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_neighbours(text, n=5):\n",
    "    words = []\n",
    "    text = [re.sub('[,.?!\"\":;+={}()&*^%$#@/]', '', t).lower() for t in text.split()]\n",
    "    for word in text:\n",
    "        for i in range(n, len(word)):\n",
    "            words.append(word[:i])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('queries_test.txt') as f:\n",
    "    queries = f.readlines()\n",
    "queries = list(map(lambda x: x[:-1].split('\\t')[1], queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37013591549\n"
     ]
    }
   ],
   "source": [
    "score = []\n",
    "with open('train.txt') as ft:\n",
    "    with open('submission.txt', 'w') as fb:\n",
    "        ft.readline()\n",
    "        fb.write('QueryId,DocumentId\\n')\n",
    "\n",
    "        qids, docids, relevs, true_relevs = [], [], [], []\n",
    "        for line in ft:\n",
    "            split = line.split(',')\n",
    "            rel, qid, docid = int(split[0]), int(split[1]), int(split[2])\n",
    "            url = ''.join(split[3:])\n",
    "            qid, docid = int(qid), int(docid)\n",
    "            true_relevs.append(rel)\n",
    "            \n",
    "            if len(qids) > 0 and qid != qids[-1]:\n",
    "                \n",
    "                order = np.argsort(relevs)[::-1]\n",
    "                docids = np.array(docids)[order]\n",
    "#                 print(ndcg_score(true_relevs, np.array(true_relevs)[order]))\n",
    "                score.append(ndcg_score(true_relevs, np.array(true_relevs)[order]))\n",
    "\n",
    "                for i in range(len(qids)):\n",
    "                    fb.write('{},{}\\n'.format(qids[i], docids[i]))\n",
    "                    \n",
    "                qids, docids, relevs, true_relevs = [], [], [], []\n",
    "            try:\n",
    "                page = urlopen(url, timeout=10)\n",
    "                soup = BeautifulSoup(page, \"html.parser\")\n",
    "                for elem in soup.find_all(['style', 'script', '[document]', 'head', 'title']):\n",
    "                    elem.extract()\n",
    "#                 text = word_neighbours(' '.join(BeautifulSoup(urlopen(url, timeout=10), 'lxml').stripped_strings))\n",
    "                text = word_neighbours(soup.getText())\n",
    "            except OSError:\n",
    "                continue\n",
    "            frec = 0\n",
    "            for word in word_neighbours(queries[qid - 1]):\n",
    "                frec += np.mean([t == word for t in text])\n",
    "                \n",
    "            break\n",
    "            qids.append(qid)\n",
    "            docids.append(docid)\n",
    "            relevs.append(frec)\n",
    "            \n",
    "print(np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "score = []\n",
    "with open('test.txt') as ft:\n",
    "    with open('submission.txt', 'w') as fb:\n",
    "        ft.readline()\n",
    "        fb.write('QueryId,DocumentId\\n')\n",
    "\n",
    "        qids, docids, relevs = [], [], []\n",
    "        counter = 0\n",
    "        for line in ft:\n",
    "            split = line.split(',')\n",
    "            qid, docid = int(split[0]), int(split[1])\n",
    "            url = ''.join(split[2:])\n",
    "            qid, docid = int(qid), int(docid)\n",
    "            \n",
    "            if len(qids) > 0 and qid != qids[-1]:\n",
    "                order = np.argsort(relevs)[::-1]\n",
    "                docids = np.array(docids)[order]\n",
    "                print(counter)\n",
    "                counter += 1\n",
    "                \n",
    "                for i in range(len(qids)):\n",
    "                    fb.write('{},{}\\n'.format(qids[i], docids[i]))\n",
    "                    \n",
    "                qids, docids, relevs = [], [], []\n",
    "            try:\n",
    "                page = urlopen(url, timeout=10)\n",
    "                soup = BeautifulSoup(page, \"html.parser\")\n",
    "                for elem in soup.find_all(['style', 'script', '[document]', 'head', 'title']):\n",
    "                    elem.extract()\n",
    "#                 text = word_neighbours(' '.join(BeautifulSoup(urlopen(url, timeout=10), 'lxml').stripped_strings))\n",
    "                text = word_neighbours(soup.getText())\n",
    "            except OSError:\n",
    "                continue\n",
    "            frec = 0\n",
    "            for word in word_neighbours(queries[qid - 101]):\n",
    "                frec += np.mean([t == word for t in text])\n",
    "                \n",
    "            qids.append(qid)\n",
    "            docids.append(docid)\n",
    "            relevs.append(frec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
